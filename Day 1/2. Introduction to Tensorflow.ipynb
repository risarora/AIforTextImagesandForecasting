{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Currently, the most famous deep learning library in the world is Google's TensorFlow. Google product uses machine learning in all of its products to improve the search engine, translation, image captioning or recommendations.\n",
    "\n",
    "To give a concrete example, Google users can experience a faster and more refined the search with AI. If the user types a keyword a the search bar, Google provides a recommendation about what could be the next word.\n",
    "\n",
    "\n",
    "\n",
    "Google wants to use machine learning to take advantage of their massive datasets to give users the best experience. Three different groups use machine learning:\n",
    "\n",
    "Researchers\n",
    "Data scientists\n",
    "Programmers.\n",
    "They can all use the same toolset to collaborate with each other and improve their efficiency.\n",
    "\n",
    "Google does not just have any data; they have the world's most massive computer, so Tensor Flow was built to scale. TensorFlow is a library developed by the Google Brain Team to accelerate machine learning and deep neural network research.\n",
    "\n",
    "It was built to run on multiple CPUs or GPUs and even mobile operating systems, and it has several wrappers in several languages like Python, C++ or Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Architecture\n",
    "#### Tensorflow architecture works in three parts:\n",
    "\n",
    "* Preprocessing the data\n",
    "* Build the model\n",
    "* Train and estimate the model\n",
    "It is called Tensorflow because it takes input as a multi-dimensional array, also known as tensors. You can construct a sort of flowchart of operations (called a Graph) that you want to perform on that input. The input goes in at one end, and then it flows through this system of multiple operations and comes out the other end as output.\n",
    "\n",
    "This is why it is called TensorFlow because the tensor goes in it flows through a list of operations, and then it comes out the other side.\n",
    "\n",
    "### Where can Tensorflow run?\n",
    "\n",
    "![./images/tensorflowFeatures.png](./images/tensorflowFeatures.png)\n",
    "\n",
    "TensorFlow can hardware, and software requirements can be classified into\n",
    "\n",
    "Development Phase: This is when you train the mode. Training is usually done on your Desktop or laptop.\n",
    "\n",
    "Run Phase or Inference Phase: Once training is done Tensorflow can be run on many different platforms. You can run it on\n",
    "\n",
    "* Desktop running Windows, macOS or Linux\n",
    "* Cloud as a web service\n",
    "* Mobile devices like iOS and Android\n",
    "You can train it on multiple machines then you can run it on a different machine, once you have the trained model.\n",
    "\n",
    "The model can be trained and used on GPUs as well as CPUs. GPUs were initially designed for video games. In late 2010, Stanford researchers found that GPU was also very good at matrix operations and algebra so that it makes them very fast for doing these kinds of calculations. Deep learning relies on a lot of matrix multiplication. TensorFlow is very fast at computing the matrix multiplication because it is written in C++. Although it is implemented in C++, TensorFlow can be accessed and controlled by other languages mainly, Python.\n",
    "\n",
    "Finally, a significant feature of TensorFlow is the TensorBoard. The TensorBoard enables to monitor graphically and visually what TensorFlow is doing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![./images/tensorflowlibraryandextensions.png](./images/tensorflowlibraryandextensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "![./images/tensorboard.png](./images/tensorboard.png)\n",
    "![./images/tensorboard.gif](./images/tensorboard.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "![./images/datasets.png](./images/datasets.png)\n",
    "\n",
    "###  Datasets belong to these categories more than a hundred datasets\n",
    "* Audio\n",
    "* Image\n",
    "* Structured\n",
    "* Video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Hub\n",
    "![./images/tfhub.png](./images/tfhub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving Models\n",
    "![./images/tfserving.png](./images/tfserving.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "![./images/modeloptimization.png](images/modeloptimization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "![./images/probability.png](./images/probability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Federated\n",
    "\n",
    "![./images/tfFederated.png](./images/tfFederated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLIR\n",
    "\n",
    "![./images/mlir.png](./images/mlir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Structured Learning\n",
    "![./images/neuralStrucuredLearning.png](./images/neuralStrucuredLearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLA\n",
    "\n",
    "![./images/XLA.png](./images/XLA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TensorFlow Graphics\n",
    "\n",
    "\n",
    "The last few years have seen a rise in novel differentiable graphics layers\n",
    "which can be inserted in neural network architectures. From spatial transformers\n",
    "to differentiable graphics renderers, these new layers leverage the knowledge\n",
    "acquired over years of computer vision and graphics research to build new and\n",
    "more efficient network architectures. Explicitly modeling geometric priors and\n",
    "constraints into neural networks opens up the door to architectures that can be\n",
    "trained robustly, efficiently, and more importantly, in a self-supervised\n",
    "fashion.\n",
    "\n",
    "## Overview\n",
    "\n",
    "At a high level, a computer graphics pipeline requires a representation of 3D\n",
    "objects and their absolute positioning in the scene, a description of the\n",
    "material they are made of, lights and a camera. This scene description is then\n",
    "interpreted by a renderer to generate a synthetic rendering.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img border=\"0\"  src=\"./images/graphics.jpg\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "In comparison, a computer vision system would start from an image and try to\n",
    "infer the parameters of the scene. This allows the prediction of which objects\n",
    "are in the scene, what materials they are made of, and their three-dimensional\n",
    "position and orientation.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img border=\"0\"  src=\"https://storage.googleapis.com/tensorflow-graphics/git/readme/cv.jpg\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "Training machine learning systems capable of solving these complex 3D vision\n",
    "tasks most often requires large quantities of data. As labelling data is a\n",
    "costly and complex process, it is important to have mechanisms to design machine\n",
    "learning models that can comprehend the three dimensional world while being\n",
    "trained without much supervision. Combining computer vision and computer\n",
    "graphics techniques provides a unique opportunity to leverage the vast amounts\n",
    "of readily available unlabelled data. As illustrated in the image below, this\n",
    "can, for instance, be achieved using analysis by synthesis where the vision\n",
    "system extracts the scene parameters and the graphics system renders back an\n",
    "image based on them. If the rendering matches the original image, the vision\n",
    "system has accurately extracted the scene parameters. In this setup, computer\n",
    "vision and computer graphics go hand in hand, forming a single machine learning\n",
    "system similar to an autoencoder, which can be trained in a self-supervised\n",
    "manner.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img border=\"0\"  src=\"https://storage.googleapis.com/tensorflow-graphics/git/readme/cv_graphics.jpg\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "Tensorflow Graphics is being developed to help tackle these types of challenges\n",
    "and to do so, it provides a set of differentiable graphics and geometry layers\n",
    "(e.g. cameras, reflectance models, spatial transformations, mesh convolutions)\n",
    "and 3D viewer functionalities (e.g. 3D TensorBoard) that can be used to train\n",
    "and debug your machine learning models of choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIG Addons\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./images/SIGAddons.png\" width=\"60%\"><br><br>\n",
    "</div>\n",
    "\n",
    "\n",
    "**TensorFlow Addons** is a repository of contributions that conform to\n",
    "well-established API patterns, but implement new functionality\n",
    "not available in core TensorFlow. TensorFlow natively supports\n",
    "a large number of operators, layers, metrics, losses, and optimizers.\n",
    "However, in a fast moving field like ML, there are many interesting new\n",
    "developments that cannot be integrated into core TensorFlow\n",
    "(because their broad applicability is not yet clear, or it is mostly\n",
    " used by a smaller subset of the community).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIG IO\n",
    "\n",
    "TensorFlow I/O is a collection of file systems and file formats that are not\n",
    "available in TensorFlow's built-in support.\n",
    "\n",
    "At the moment TensorFlow I/O supports the following data sources:\n",
    "- `tensorflow_io.ignite`: Data source for Apache Ignite and Ignite File System (IGFS). Overview and usage guide [here](tensorflow_io/ignite/README.md).\n",
    "- `tensorflow_io.kafka`: Apache Kafka stream-processing support.\n",
    "- `tensorflow_io.kinesis`: Amazon Kinesis data streams support.\n",
    "- `tensorflow_io.hadoop`: Hadoop SequenceFile format support.\n",
    "- `tensorflow_io.arrow`: Apache Arrow data format support. Usage guide [here](tensorflow_io/arrow/README.md).\n",
    "- `tensorflow_io.image`: WebP and TIFF image format support.\n",
    "- `tensorflow_io.libsvm`: LIBSVM file format support.\n",
    "- `tensorflow_io.ffmpeg`: Video and Audio file support with FFmpeg.\n",
    "- `tensorflow_io.parquet`: Apache Parquet data format support.\n",
    "- `tensorflow_io.lmdb`: LMDB file format support.\n",
    "- `tensorflow_io.mnist`: MNIST file format support.\n",
    "- `tensorflow_io.pubsub`: Google Cloud Pub/Sub support.\n",
    "- `tensorflow_io.bigtable`: Google Cloud Bigtable support.\n",
    "- `tensorflow_io.oss`: Alibaba Cloud Object Storage Service (OSS) support. Usage guide [here](https://github.com/tensorflow/io/blob/master/tensorflow_io/oss/README.md).\n",
    "- `tensorflow_io.avro`: Apache Avro file format support.\n",
    "- `tensorflow_io.audio`: WAV file format support.\n",
    "- `tensorflow_io.grpc`: gRPC server Dataset, support for streaming Numpy input.\n",
    "- `tensorflow_io.hdf5`: HDF5 file format support.\n",
    "- `tensorflow_io.text`: Text file with archive support.\n",
    "- `tensorflow_io.pcap`: Pcap network packet capture file support.\n",
    "- `tensorflow_io.azure`: Microsoft Azure Storage support. Usage guide [here](https://github.com/tensorflow/io/blob/master/tensorflow_io/azure/README.md).\n",
    "- `tensorflow_io.bigquery`: Google Cloud BigQuery support.\n",
    "- `tensorflow_io.gcs`: GCS Configuration support.\n",
    "- `tensorflow_io.prometheus`: Prometheus observation data support.\n",
    "- `tensorflow_io.dicom`: DICOM Image file format support. Usage guide [here](https://github.com/tensorflow/io/blob/master/tensorflow_io/dicom/README.md).\n",
    "- `tensorflow_io.json`: JSON file support. Usage guide [here](https://github.com/tensorflow/io/blob/master/tensorflow_io/json/README.md).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current latest version is 2.0 compatiable with python 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU and TPU scalability\n",
    "\n",
    "https://www.tensorflow.org/guide/distributed_training\n",
    "\n",
    "Distributed training with TensorFlow\n",
    "## Overview\n",
    "\n",
    "`tf.distribute.Strategy` is a TensorFlow API to distribute training \n",
    "across multiple GPUs, multiple machines or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
    "\n",
    "`tf.distribute.Strategy` has been designed with these key goals in mind:\n",
    "\n",
    "* Easy to use and support multiple user segments, including researchers, ML engineers, etc.\n",
    "* Provide good performance out of the box.\n",
    "* Easy switching between strategies.\n",
    "\n",
    "Use `tf.distribute.Strategy` with a high-level API like [Keras](https://www.tensorflow.org/guide/keras), and can also be used to distribute custom training loops (and, in general, any computation using TensorFlow).\n",
    "\n",
    "In TensorFlow 2.0, you can execute your programs eagerly, or in a graph using [`tf.function`](../tutorials/eager/tf_function.ipynb). `tf.distribute.Strategy` intends to support both these modes of execution. Although we discuss training most of the time in this guide, this API can also be used for distributing evaluation and prediction on different platforms.\n",
    "\n",
    "You can use `tf.distribute.Strategy` with very few changes to your code, because we have changed the underlying components of TensorFlow to become strategy-aware. This includes variables, layers, models, optimizers, metrics, summaries, and checkpoints.\n",
    "\n",
    "In this guide, we explain various types of strategies and how you can use them in different situations.\n",
    "\n",
    "Note: For a deeper understanding of the concepts, please watch [this deep-dive presentation](https://youtu.be/jKV53r9-H14). This is\n",
    "especially recommended if you plan to write your own training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of strategies\n",
    "`tf.distribute.Strategy` intends to cover a number of use cases along different axes. Some of these axes are:\n",
    "\n",
    "* *Synchronous vs asynchronous training:* These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.\n",
    "* *Hardware platform:* You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.\n",
    "\n",
    "In order to support these use cases, there are five strategies available. In the next section we explain which of these are supported in which scenarios in TF 2.0 at this time. Here is a quick overview:\n",
    "\n",
    "\n",
    "## Stratergies Supported\n",
    "\n",
    "* **OneDeviceStrategy**   \n",
    "  runs on a single device. This strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via strategy.experimental_run_v2 will also be placed on the specified device.\n",
    "\n",
    "* **TPUStrategy**   \n",
    "  lets you run your TensorFlow training on Tensor Processing Units (TPUs). TPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads. They are available on Google Colab, the TensorFlow Research Cloud and Cloud TPU.\n",
    "\n",
    "* **MirroredStrategy**   \n",
    "  supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas.\n",
    "* **MultiWorkerMirroredStrategy**   \n",
    "  is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
    "* **CentralStorageStrategy**   \n",
    "  does synchronous training as well. Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.\n",
    "* **ParameterServerStrategy**   \n",
    "  supports parameter servers training on multiple machines. In this setup, some machines are designated as workers and some as parameter servers. Each variable of the model is placed on one parameter server. Computation is replicated across all GPUs of all the workers.\n",
    "\n",
    "\n",
    "Note: Estimator support is limited. Basic training and evaluation are experimental, and advanced features—such as scaffold—are not implemented. We\n",
    "recommend using Keras or custom training loops if a use case is not covered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Components of TensorFlow\n",
    "#### Tensor\n",
    "\n",
    "Tensorflow's name is directly derived from its core framework: Tensor. In Tensorflow, all the computations involve tensors. A tensor is a vector or matrix of n-dimensions that represents all types of data. All values in a tensor hold identical data type with a known (or partially known) shape. The shape of the data is the dimensionality of the matrix or array.\n",
    "\n",
    "A tensor can be originated from the input data or the result of a computation. In TensorFlow, all the operations are conducted inside a graph. The graph is a set of computation that takes place successively. Each operation is called an op node and are connected to each other.\n",
    "\n",
    "The graph outlines the ops and connections between the nodes. However, it does not display the values. The edge of the nodes is the tensor, i.e., a way to populate the operation with data.\n",
    "\n",
    "#### Graphs\n",
    "\n",
    "TensorFlow makes use of a graph framework. The graph gathers and describes all the series computations done during the training. The graph has lots of advantages:\n",
    "\n",
    "* It was done to run on multiple CPUs or GPUs and even mobile operating system\n",
    "* The portability of the graph allows to preserve the computations for immediate or later use. The graph can be saved to be executed in the future.\n",
    "* All the computations in the graph are done by connecting tensors together\n",
    "* A tensor has a node and an edge. The node carries the mathematical operation and produces an endpoints outputs. The edges the edges explain the input/output relationships between nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = tf.placeholder(tf.float32, name = \"X_1\")\n",
    "X_2 = tf.placeholder(tf.float32, name = \"X_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply = tf.multiply(X_1, X_2, name = \"multiply\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Execute the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. 10. 18.]\n"
     ]
    }
   ],
   "source": [
    "X_1 = tf.placeholder(tf.float32, name = \"X_1\")\n",
    "X_2 = tf.placeholder(tf.float32, name = \"X_2\")\n",
    "\n",
    "multiply = tf.multiply(X_1, X_2, name = \"multiply\")\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(multiply, feed_dict={X_1:[1,2,3], X_2:[4,5,6]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options to Load Data into TensorFlow\n",
    "The first step before training a machine learning algorithm is to load the data. There is two commons way to load data:\n",
    "\n",
    "1. **Load data into memory:**    \n",
    "It is the simplest method. You load all your data into memory as a single array. You can write a Python code. This lines of code are unrelated to Tensorflow.\n",
    "\n",
    "2. **Tensorflow data pipeline.**   \n",
    "Tensorflow has built-in API that helps you to load the data, perform the operation and feed the machine learning algorithm easily. This method works very well especially when you have a large dataset. For instance, image records are known to be enormous and do not fit into memory. The data pipeline manages the memory by itself\n",
    "\n",
    "### What solution to use?\n",
    "\n",
    "* Load data in memory\n",
    "\n",
    "If your dataset is not too big, i.e., less than 10 gigabytes, you can use the first method. The data can fit into the memory. You can use a famous library called Pandas to import CSV files. You will learn more about pandas in the next tutorial.\n",
    "\n",
    "* Load data with Tensorflow pipeline\n",
    "\n",
    "The second method works best if you have a large dataset. For instance, if you have a dataset of 50 gigabytes, and your computer has only 16 gigabytes of memory then the machine will crash.\n",
    "\n",
    "In this situation, you need to build a Tensorflow pipeline. The pipeline will load the data in batch, or small chunk. Each batch will be pushed to the pipeline and be ready for the training. Building a pipeline is an excellent solution because it allows you to use parallel computing. It means Tensorflow will train the model across multiple CPUs. It fosters the computation and permits for training powerful neural network.\n",
    "\n",
    "You will see in the next tutorials on how to build a significant pipeline to feed your neural network.\n",
    "\n",
    "In a nutshell, if you have a small dataset, you can load the data in memory with Pandas library.\n",
    "\n",
    "If you have a large dataset and you want to make use of multiple CPUs, then you will be more comfortable to work with Tensorflow pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tensorflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26709538 0.66948476]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x_input = np.random.sample((1,2))\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the placeholder\n",
    "Like in the previous example, we create a placeholder with the name X. We need to specify the shape of the tensor explicitly. In case, we will load an array with only two values. We can write the shape as shape=[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[1,2], name = 'X')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the dataset method\n",
    "\n",
    "next, we need to define the Dataset where we can populate the value of the placeholder x. We need to use the method tf.data.Dataset.from_tensor_slices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the pipeline\n",
    "\n",
    "In step four, we need to initialize the pipeline where the data will flow. We need to create an iterator with make_initializable_iterator. We name it iterator. Then we need to call this iterator to feed the next batch of data, get_next. We name this step get_next. Note that in our example, there is only one batch of data with only two values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/envs/bert/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_initializable_iterator() \n",
    "get_next = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Execute the operation\n",
    "\n",
    "The last step is similar to the previous example. We initiate a session, and we run the operation iterator. We feed the feed_dict with the value generated by numpy. These two value will populate the placeholder x. Then we run get_next to print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2670954  0.66948473]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # feed the placeholder with data\n",
    "    sess.run(iterator.initializer, feed_dict={ x: x_input }) \n",
    "    print(sess.run(get_next)) # output [ 0.52374458  0.71968478]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
